{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__\n",
    "import numpy as np\n",
    "np.__version__\n",
    "from keras.datasets import reuters as dataset\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j3nnn1/anaconda3/envs/nlp_deep/lib/python3.7/site-packages/keras/datasets/reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/j3nnn1/anaconda3/envs/nlp_deep/lib/python3.7/site-packages/keras/datasets/reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "# Primer hyperpar√°metro\n",
    "num_words=30000\n",
    "\n",
    "(training_data, training_targets), (testing_data, testing_targets) = dataset.load_data(num_words=num_words+2)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12])\n",
      " list([1, 3267, 699, 3434, 2295, 56, 16784, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 19261, 49, 2295, 13415, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 13415, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12])\n",
      " list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 11733, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 13051, 71, 5, 160, 63, 11, 9, 26503, 81, 5, 102, 59, 11, 17, 12])\n",
      " ...\n",
      " list([1, 1809, 124, 53, 653, 26, 39, 5439, 18, 14, 5893, 18, 155, 177, 53, 544, 26, 39, 19, 5121, 18, 14, 19, 6382, 18, 280, 3882, 11, 14, 3123, 32, 11, 695, 3614, 47, 11, 14, 3615, 63, 11, 430, 3259, 44, 11, 14, 61, 11, 17, 12])\n",
      " list([1, 5586, 15148, 71, 8, 23, 166, 344, 10, 78, 13, 68, 80, 467, 606, 6, 261, 5, 146, 93, 124, 4, 166, 75, 3603, 14296, 5907, 265, 8692, 1251, 14144, 297, 1127, 195, 9, 621, 575, 1080, 5907, 7, 378, 104, 421, 648, 20, 5, 4, 49, 14144, 8, 1708, 28, 4, 303, 163, 524, 10, 1220, 6, 455, 4, 326, 685, 6, 15081, 422, 71, 142, 73, 863, 62, 75, 3603, 6, 4, 326, 166, 14144, 34, 1652, 3603, 6, 4, 166, 4, 49, 8, 17, 12])\n",
      " list([1, 706, 209, 658, 4, 37, 38, 309, 484, 4, 1434, 6, 933, 4, 89, 709, 377, 101, 28, 4, 143, 511, 101, 5, 47, 758, 15, 90, 2388, 7, 809, 6, 444, 2035, 4, 911, 5, 709, 198, 1997, 634, 3644, 3798, 2305, 8, 1486, 6, 674, 480, 10, 990, 309, 4008, 2190, 2305, 1849, 24, 68, 583, 242, 5, 4, 143, 709, 364, 7376, 41, 30, 13, 706, 6, 837, 4, 377, 101, 6, 631, 28, 47, 758, 15, 36, 1413, 107, 4, 377, 101, 62, 47, 758, 15, 634, 114, 713, 888, 1412, 6, 343, 37, 38, 1116, 95, 1136, 269, 43, 1488, 1170, 6, 226, 11435, 4, 377, 101, 136, 143, 1032, 4, 89, 709, 377, 101, 1217, 30, 478, 97, 47, 948, 15, 90, 4594, 27184, 5853, 41, 30, 13, 706, 6, 455, 4, 465, 474, 6, 837, 634, 6, 2069, 4, 709, 377, 101, 28, 47, 758, 15, 7, 463, 29, 89, 1017, 97, 148, 16, 6, 47, 948, 15, 4, 48, 511, 377, 101, 23, 47, 758, 15, 161, 5, 4, 47, 12, 20, 7424, 7978, 386, 240, 2305, 2634, 24, 10, 181, 1475, 7, 194, 534, 21, 709, 364, 756, 33, 30, 4, 386, 404, 36, 118, 4, 2190, 24, 4, 911, 7, 1116, 23, 24, 4, 37, 38, 377, 101, 1976, 42, 9964, 6, 127, 122, 9, 7609, 1136, 692, 13, 37, 38, 1116, 446, 69, 4, 234, 709, 7614, 1320, 13, 126, 1006, 5, 338, 458, 2305, 8, 4, 1136, 911, 23, 4, 307, 2016, 36, 8, 634, 23, 325, 2863, 4, 820, 9, 129, 2767, 40, 836, 85, 1523, 17, 12])]\n",
      "\n",
      "\n",
      "# of Rows: {} 11228\n",
      "# of Classes: {} 46\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print('\\n')\n",
    "print('# of Rows: {}', len(data))\n",
    "print('# of Classes: {}', max(training_targets) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>[1, 9777, 9805, 71, 8, 25, 9455, 71, 323, 23, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "count                                               11228\n",
       "unique                                              10439\n",
       "top     [1, 9777, 9805, 71, 8, 25, 9455, 71, 323, 23, ...\n",
       "freq                                                    6"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "type(data)\n",
    "#narray to dataframe\n",
    "dfData = pd.DataFrame(data)\n",
    "type(dfData)\n",
    "dfData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11228.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.890720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.153068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  11228.000000\n",
       "mean       8.890720\n",
       "std        9.153068\n",
       "min        0.000000\n",
       "25%        3.000000\n",
       "50%        4.000000\n",
       "75%       14.000000\n",
       "max       45.000000"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "type(targets)\n",
    "#narray to dataframe\n",
    "dfTarget = pd.DataFrame(targets)\n",
    "type(dfTarget)\n",
    "dfTarget.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45]\n",
      "Number of unique words: 30000\n"
     ]
    }
   ],
   "source": [
    "# Tengo 46 categor√≠as: \n",
    "num_categories = len(np.unique(targets))\n",
    "print(\"Categories:\", np.unique(targets))\n",
    "# Tengo num_words palabras √∫nicas en el vocabulario\n",
    "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Review length: 145.96419665122906\n",
      "Standard Deviation: 146\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Longitudes promedio de los comentarios de las pel√≠culas\n",
    "length = [len(i) for i in data]\n",
    "print(\"Average Review length:\", np.mean(length))\n",
    "print(\"Standard Deviation:\", round(np.std(length)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n",
      "[1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 465, 893, 3541, 114, 2902, 69, 312, 35, 15, 7, 335, 1679, 21, 25, 3675, 19519, 3498, 58, 69, 68, 493, 5, 25, 465, 377, 2430, 4, 293, 1172, 739, 4379, 8, 7, 1510, 1131, 13, 899, 6, 4, 990, 309, 415, 4519, 6920, 645, 3916, 791, 5, 4379, 75, 8, 24, 10, 1311, 4677, 5, 344, 756, 7, 29700, 231, 9691, 2603, 1413, 43, 509, 43, 68, 327, 5, 14560, 3498, 297, 638, 73, 430, 22, 4, 580, 7, 48, 41, 30, 14021, 136, 4, 344, 298, 4, 580, 40, 344, 5078, 23457, 291, 1488, 10, 3148, 5, 231, 6250, 1308, 5, 8250, 7043, 21, 18776, 1622, 990, 309, 415, 265, 5992, 8945, 1149, 9118, 27677, 4, 344, 9691, 756, 3729, 14560, 4667, 28400, 3249, 28, 10, 2190, 24, 77, 41, 682, 10, 4851, 2048, 7, 4, 5540, 2926, 1598, 22, 370, 5954, 7541, 5, 54, 5232, 1685, 2916, 10, 1571, 946, 60, 51, 3249, 5249, 4, 73, 2135, 669, 4, 580, 64, 10, 4280, 6, 16319, 25, 482, 35, 150, 377, 2430, 7, 10, 21743, 836, 29981, 4730, 6920, 5, 4379, 12711, 16799, 3541, 8, 4, 344, 291, 29693, 298, 4228, 6, 2223, 24, 14560, 41, 343, 430, 210, 6, 3498, 297, 64, 10, 2281, 455, 5, 7003, 125, 222, 17, 12]\n",
      "tiene N %s palabras (224,)\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "print(\"Label:\", targets[i])\n",
    "# Las comentarios ya est√°n preprocesados\n",
    "print(data[i])\n",
    "print(\"tiene N %s palabras\", np.shape(data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 46\n",
    "import tensorflow as tf\n",
    "y_all_one_hot_enc = tf.keras.utils.to_categorical(targets, num_classes)\n",
    "print(y_all_one_hot_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "print(len(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "def vectorize_sequences(sequences, dimension=30002):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results\n",
    "\n",
    "\n",
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 46\n",
    "import tensorflow as tf\n",
    "y_all_one_hot_enc = tf.keras.utils.to_categorical(targets, num_classes)\n",
    "print(y_all_one_hot_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11228, 46)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_all_one_hot_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = vectorize_sequences(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87,)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11228, 30002)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30002"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30002"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "salida_densa = 350\n",
    "input_shape = len(train[0])\n",
    "#input_shape = max num words \n",
    "\n",
    "model.add(Dense(salida_densa, input_shape=(input_shape,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_97 (Dense)             (None, 350)               10501050  \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 350)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 64)                22464     \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 46)                2990      \n",
      "=================================================================\n",
      "Total params: 10,526,504\n",
      "Trainable params: 10,526,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Lets explore the model\n",
    "# http://keras.io/models/about-keras-models/\n",
    "print(model.summary()) #  summary representation of your model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30002,)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "np.shape(y_all_one_hot_enc[0])\n",
    "print(y_all_one_hot_enc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = train[:1000]\n",
    "partial_x_train = train[1000:]\n",
    "y_val = y_all_one_hot_enc[:1000]\n",
    "partial_y_train = y_all_one_hot_enc[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/j3nnn1/anaconda3/envs/nlp_deep/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/10\n",
      "11228/11228 [==============================] - 46s 4ms/step - loss: 1.3072 - acc: 0.7105\n",
      "Epoch 2/10\n",
      "11228/11228 [==============================] - 44s 4ms/step - loss: 0.7296 - acc: 0.8424\n",
      "Epoch 3/10\n",
      "11228/11228 [==============================] - 44s 4ms/step - loss: 0.5350 - acc: 0.8855\n",
      "Epoch 4/10\n",
      "11228/11228 [==============================] - 44s 4ms/step - loss: 0.4318 - acc: 0.9098\n",
      "Epoch 5/10\n",
      "11228/11228 [==============================] - 44s 4ms/step - loss: 0.3699 - acc: 0.9241\n",
      "Epoch 6/10\n",
      "11228/11228 [==============================] - 44s 4ms/step - loss: 0.3379 - acc: 0.9311\n",
      "Epoch 7/10\n",
      "11228/11228 [==============================] - 44s 4ms/step - loss: 0.3224 - acc: 0.9347\n",
      "Epoch 8/10\n",
      "11228/11228 [==============================] - 47s 4ms/step - loss: 0.3050 - acc: 0.9378\n",
      "Epoch 9/10\n",
      "11228/11228 [==============================] - 48s 4ms/step - loss: 0.3005 - acc: 0.9396\n",
      "Epoch 10/10\n",
      "11228/11228 [==============================] - 47s 4ms/step - loss: 0.2883 - acc: 0.9439\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 10\n",
    "\n",
    "result = model.fit(train, y_all_one_hot_enc, epochs=nb_epoch,  batch_size=batch_size)\n",
    "#result = model.fit(train, y_all_one_hot_enc, epochs=nb_epoch, batch_size=batch_size, verbose=1, validation_data=(x_val, y_val))\n",
    "#result = model.fit(train, y_all_one_hot_enc, epochs=nb_epoch, batch_size=batch_size, verbose=1, validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% md\n",
    "\n",
    "# Armar una MLP usando Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armar una MLP usando Embeddings\n",
    "from keras.layers import Embedding, Flatten, Dropout\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparametro - Longitud m√°xima de comentario\n",
    "maxlen=1000\n",
    "# Cantidad de palabras totales contando las reservadas\n",
    "nb_words=num_words+3\n",
    "# Tamano del embedding. Es un hiperpar√°metro y puede modificarlo\n",
    "embed_dim=128\n",
    "salida_capa_densa = num_classes\n",
    "dropout=0.5 # Hiperpar√°metro\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embed_dim, input_length=maxlen, trainable=True))\n",
    "model.add(Flatten())\n",
    "#model.add(Dropout(dropout))\n",
    "model.add(Dense(salida_capa_densa, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 128)         3840384   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 46)                5888046   \n",
      "=================================================================\n",
      "Total params: 9,728,430\n",
      "Trainable params: 9,728,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87,)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(data,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Verificamos que todos tengan longitud 1000\n",
    "print(len(data[0]))\n",
    "print(np.array([len(d) for d in data]).var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11228/11228 [==============================] - 52s 5ms/step - loss: 1.7294 - acc: 0.5972\n",
      "Epoch 2/5\n",
      "11228/11228 [==============================] - 51s 5ms/step - loss: 0.7322 - acc: 0.8494\n",
      "Epoch 3/5\n",
      "11228/11228 [==============================] - 53s 5ms/step - loss: 0.4203 - acc: 0.9254\n",
      "Epoch 4/5\n",
      "11228/11228 [==============================] - 57s 5ms/step - loss: 0.3223 - acc: 0.9414\n",
      "Epoch 5/5\n",
      "11228/11228 [==============================] - 52s 5ms/step - loss: 0.2888 - acc: 0.9445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e27ec0e10>"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data,y_all_one_hot_enc,batch_size=32,epochs=5)#,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% md\n",
    "\n",
    "# Armar una CNN\n",
    "## Abajo hay un ejemplo de arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# _________________________________________________________________\n",
    "# Layer (type)                 Output Shape              Param #   \n",
    "# =================================================================\n",
    "# embedding_12 (Embedding)     (None, 1000, 32)          960096    \n",
    "# _________________________________________________________________\n",
    "# conv1d_7 (Conv1D)            (None, 1000, 64)          14400     \n",
    "# _________________________________________________________________\n",
    "# max_pooling1d_4 (MaxPooling1 (None, 500, 64)           0         \n",
    "# _________________________________________________________________\n",
    "# conv1d_8 (Conv1D)            (None, 500, 128)          57472     \n",
    "# _________________________________________________________________\n",
    "# global_max_pooling1d_4 (Glob (None, 128)               0         \n",
    "# _________________________________________________________________\n",
    "# dropout_4 (Dropout)          (None, 128)               0         \n",
    "# _________________________________________________________________\n",
    "# dense_19 (Dense)             (None, 46)                5934      \n",
    "# ================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras.layers import Embedding, Flatten, Dropout, Conv1D, MaxPooling1D\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "# Cantidad de palabras totales contando las reservadas\n",
    "nb_words=num_words+3\n",
    "# Tamano del embedding. Es un hiperpar√°metro y puede modificarlo\n",
    "embed_dim=128\n",
    "salida_capa_densa = num_classes\n",
    "dropout=0.5 # Hiperpar√°metro\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embed_dim, input_length=maxlen, trainable=True))\n",
    "model.add(Conv1D(filters=64, kernel_size=8, activation='relu' , input_shape = (maxlen,embed_dim)))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(salida_capa_densa, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1000, 128)         3840384   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 993, 64)           65600     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 496, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 31744)             0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 31744)             0         \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 46)                1460270   \n",
      "=================================================================\n",
      "Total params: 5,366,254\n",
      "Trainable params: 5,366,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIQUE HYPERPARAMS A GUSTO\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/10\n",
      "8982/8982 [==============================] - 101s 11ms/step - loss: 0.5233 - acc: 0.8661 - val_loss: 1.4480 - val_acc: 0.7164\n",
      "Epoch 2/10\n",
      "8982/8982 [==============================] - 102s 11ms/step - loss: 0.4393 - acc: 0.8874 - val_loss: 1.5220 - val_acc: 0.7168\n",
      "Epoch 3/10\n",
      "8982/8982 [==============================] - 102s 11ms/step - loss: 0.3761 - acc: 0.9028 - val_loss: 1.6108 - val_acc: 0.7164\n",
      "Epoch 4/10\n",
      "8982/8982 [==============================] - 102s 11ms/step - loss: 0.3244 - acc: 0.9135 - val_loss: 1.6230 - val_acc: 0.7244\n",
      "Epoch 5/10\n",
      "8982/8982 [==============================] - 103s 11ms/step - loss: 0.2834 - acc: 0.9226 - val_loss: 1.6949 - val_acc: 0.7177\n",
      "Epoch 6/10\n",
      "8982/8982 [==============================] - 104s 12ms/step - loss: 0.2500 - acc: 0.9330 - val_loss: 1.7379 - val_acc: 0.7168\n",
      "Epoch 7/10\n",
      "8982/8982 [==============================] - 103s 11ms/step - loss: 0.2293 - acc: 0.9382 - val_loss: 1.7348 - val_acc: 0.7146\n",
      "Epoch 8/10\n",
      "8982/8982 [==============================] - 103s 11ms/step - loss: 0.2027 - acc: 0.9434 - val_loss: 1.7608 - val_acc: 0.7155\n",
      "Epoch 9/10\n",
      "8982/8982 [==============================] - 105s 12ms/step - loss: 0.1888 - acc: 0.9464 - val_loss: 1.8033 - val_acc: 0.7155\n",
      "Epoch 10/10\n",
      "8982/8982 [==============================] - 102s 11ms/step - loss: 0.1735 - acc: 0.9463 - val_loss: 1.8153 - val_acc: 0.7137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e2864ba90>"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data,y_all_one_hot_enc,batch_size=32,epochs=10,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
